{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "604cec88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing dp_challange_web_scraper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile 'dp_challange_web_scraper.py'\n",
    "#### html_content = requests.get(url).text\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "import random\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import urllib.request\n",
    "import requests\n",
    "import re\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "class html_scraper():\n",
    "    \n",
    "    def __init__(self):\n",
    "       \n",
    "        self.scraperapi = \"\"\"'insert api key generate by \n",
    "        http://api.scraperapi.com'\"\"\"\n",
    "        wd = '/ava_scraper'\n",
    "        \n",
    "        data_dir = os.getcwd()+'/data/'\n",
    "        print(f'dat dir = :-) {data_dir}')\n",
    "        failed = 'failed.txt'\n",
    "        if wd not in os.getcwd():\n",
    "            wd = os.getcwd()+wd\n",
    "            os.mkdir(wd)\n",
    "            os.chdir(wd)\n",
    "        if data_dir not in list(os.scandir()):\n",
    "            if not os.path.exists(data_dir):\n",
    "                os.mkdir(data_dir)\n",
    "            with open(data_dir+failed, 'w') as fid:\n",
    "                fid.write('FAILED PAGE SCRAPES')\n",
    "\n",
    "        self.fid = 'full_dp_challenge_dot_com_data_dict'\n",
    "        self.data_dict = { }\n",
    "        self.img_url_base = 'https://www.dpchallenge.com/image.php?IMAGE_ID='\n",
    "        self.scrape_type = None\n",
    "\n",
    "\n",
    "\n",
    "        with open(data_dir+'/failed.txt','r') as fid:\n",
    "            self.Request_failed = fid.read()\n",
    "        for fid in [fi_d.name for fi_d in os.scandir()]:\n",
    "            if self.fid in fid:\n",
    "                with open(fid,'r') as fid:\n",
    "                    print('data loaded')\n",
    "                    self.data_dict = json.load(fid)\n",
    "        if 'data_dict' not in vars(self):\n",
    "            self.data_dict = { }\n",
    "        \n",
    "        print(f'len data dict = {len(self.data_dict)}')\n",
    "        self.get_new()\n",
    "            \n",
    "        \n",
    "    def image_url(self,page):\n",
    "    # Parse the html content\n",
    "        base = 'https://www.dpchallenge.com'\n",
    "        url = page\n",
    "        im_id = page.split('=')[-1]\n",
    "        payload = {'api_key': self.scraperapi, 'url':url}\n",
    "        response = requests.get('http://api.scraperapi.com', params=payload).text\n",
    "        soup = BeautifulSoup(response, 'html.parser')\n",
    "        image_soup = soup.find_all('img')\n",
    "        image_src = iter({i['src'] for i in image_soup if im_id in i['src']})\n",
    "        metrics = soup.find_all('td',{'class':\"textsm\", 'valign':\"top\"})\n",
    "        metrics = [\n",
    "            i.get_text() for i in metrics if 'Avg' in i.get_text()\n",
    "                    ][0].split('\\n')\n",
    "        im_title = soup.title.get_text()\n",
    "        metrics_dict = {i.split(':')[0]:i.split(':')[1] for i in metrics if ''!=i}\n",
    "        metrics_dict['title'] = soup.title.get_text()\n",
    "        metrics_dict['author_comments'] = soup.find_all('td', \n",
    "                                        {'class':'textsm',\n",
    "                                         'width':\"450\",\n",
    "                                        'valign':\"top\"\n",
    "                                        }\n",
    "                                                       )[0].get_text()\n",
    "        metrics_dict['forum comments'] = [\n",
    "            comment.get_text() for comment in soup.find_all(\n",
    "                'table',{\n",
    "                    'class':\"forum-post\"\n",
    "                }\n",
    "            )\n",
    "        ]\n",
    "        metrics_dict = {im_id:metrics_dict}\n",
    "        print(metrics_dict)\n",
    "        return {'image_url':next(image_src), \n",
    "                'meta_data':metrics_dict, \n",
    "                'id':im_id}\n",
    "\n",
    "    def get_challenges(self):\n",
    "        url = \"\"\"https://www.dpchallenge.com/challenge_history.php?\n",
    "        order_by=0d&open=1&member=1&speed=1&invitational=1&show_all=1\"\"\"\n",
    "        payload = {'api_key': self.scraperapi, 'url':url}\n",
    "        response = requests.get('http://api.scraperapi.com', params=payload).text\n",
    "        soup = BeautifulSoup(response, 'html.parser')\n",
    "        links = soup.find_all('a')\n",
    "        base = 'https://www.dpchallenge.com'\n",
    "        return [base+challenge['href']+'&amp;show_full=1' for challenge in links\n",
    "                  if 'CHALLENGE_ID' in challenge['href']][::-1]\n",
    "\n",
    "    def competition_image_links(self):\n",
    "        base = 'https://www.dpchallenge.com'\n",
    "        url = self.challenge\n",
    "        payload = {'api_key': self.scraperapi, 'url':url}\n",
    "        response = requests.get('http://api.scraperapi.com', params=payload).text\n",
    "        soup = BeautifulSoup(response, 'html.parser')\n",
    "        links = soup.find_all('a')\n",
    "        return(soup,list({base+link['href'] for link in links if 'IMAGE' in link['href']}))\n",
    "\n",
    "    def get_image(self,image_dict): \n",
    "        pass #FINISH THIS CODE\n",
    "        base = 'https:'\n",
    "        url = base+kwargs['image_url']\n",
    "        payload = {'api_key': self.scraperapi, 'url':url}\n",
    "        response = requests.get('http://api.scraperapi.com', stream=True,params=payload)\n",
    "        fid_name = 'ava_images_new/'+kwargs['id']+'.jpg'\n",
    "        with open(fid_name, 'wb') as f:\n",
    "            response.raw.decode_content = True\n",
    "            shutil.copyfileobj(response.raw, f)\n",
    "\n",
    "    def to_json(self,meta_data, open_as):\n",
    "        with open(self.fid+'.json', open_as) as json_fid:\n",
    "            data_dict = json.load(json_fid)\n",
    "            try:\n",
    "                data_dict.update(kwargs['meta_data'])\n",
    "                data_dict = json.dumps(data_dict, indent = 4)\n",
    "                json_fid.seek(0)\n",
    "                json.dump(data_dict,json_fid)\n",
    "            except:\n",
    "                print('json not fully loded')\n",
    "                data_dict = json.loads(data_dict)\n",
    "                data_dict.update(kwargs['meta_data'])\n",
    "                data_dict = json.dumps(data_dict, indent = 4)\n",
    "                json_fid.seek(0)\n",
    "                json.dump(data_dict,json_fid)\n",
    "            finally:\n",
    "                print(url,' : not scraped')\n",
    "\n",
    "    def dp_write(self):    \n",
    "        with open(self.fid +'.json', 'w') as fid:\n",
    "            data = json.dump(self.data_dict,fid, sort_keys=True, indent=4)\n",
    "\n",
    "    def get_new(self):\n",
    "        self.current = np.array([i for i in self.data_dict if i[0].isdigit()])\n",
    "        print(f'challenges attepted {len(self.current)}')\n",
    "        re_scrape = np.array([\n",
    "            chall_idx for chall_idx in self.data_dict  \n",
    "            if 'fail message' in self.data_dict[chall_idx] \n",
    "            or  'error' in self.data_dict[chall_idx]])\n",
    "        self.current = np.setdiff1d(self.current, re_scrape)\n",
    "        print(f'succesfully scraped {len(self.current)} : to rescrape {len(re_scrape)}')\n",
    "\n",
    "        self.all_challenges = np.array(self.get_challenges())\n",
    "        self.challs_dict = {[split_url.split('&amp;') for split_url in challange_url.split('=')][1][0]\n",
    "          :challange_url for challange_url in self.all_challenges}\n",
    "        self.data_dict['challenge_urls']= self.challs_dict\n",
    "        self.all_id_challenges = np.array(list(self.challs_dict.keys()))\n",
    "        print('all_challenges=',len(self.all_challenges))\n",
    "        self.new_challenges = np.setdiff1d(self.all_id_challenges, self.current)\n",
    "        print(f' unscraped challenges = {len(self.new_challenges)}')\n",
    "        np.random.shuffle(self.new_challenges)\n",
    "\n",
    "\n",
    "    def parse_html(self):\n",
    "        links = [\n",
    "        html_table for html_table in \n",
    "        self.soup.find_all('td', align=\"center\", valign=\"middle\", width=\"160\")\n",
    "        ]\n",
    "        li = [\n",
    "            'no_url_available' if  html.img['src']==None else html.img['src'] \n",
    "            for html in links\n",
    "        ]\n",
    "\n",
    "        image_id = [\n",
    "            'removed' if 'removed' in id_ else id_.split('_')[-1] \n",
    "            for id_ in li\n",
    "        ]\n",
    "\n",
    "        tables = [\n",
    "            'missing' if html_element ==None else html_element.next_sibling.next_sibling\n",
    "            for html_element in self.soup.find_all(\n",
    "                'td', align=\"center\", \n",
    "                valign=\"middle\", \n",
    "                width=\"160\")]\n",
    "\n",
    "        meta = [\n",
    "            None if html.span == None else \n",
    "            html.find('td', valign=\"top\").next_sibling.next_sibling.get_text() \n",
    "            for html in tables\n",
    "        ]\n",
    "        \n",
    "        meta = [\n",
    "            ['no_values']*4 if mean==None else[\n",
    "                mean.split('\\t')[-7].strip('\\n').split(':') +\n",
    "                mean.split('\\t')[-1].strip('\\n').split(':')][0] \n",
    "             for mean in meta\n",
    "        ]\n",
    "        \n",
    "        \n",
    "\n",
    "        total_means = [\n",
    "                None if html_element.b == None\n",
    "                else list(html_element.b.next_siblings)\n",
    "                for html_element in tables\n",
    "        ]\n",
    "        total_means = [\n",
    "            0 if len(vote)==3 else \n",
    "            None if  'NavigableString' in str(type(vote[3])) \n",
    "            else vote[3].get_text()\n",
    "            for vote in total_means\n",
    "        ]\n",
    "\n",
    "        \n",
    "        title = self.soup.div.next_sibling.next_sibling.b.get_text()\n",
    "        \n",
    "        comp_data = {\n",
    "            comp_item.string:[comp_item.next_sibling.string,\n",
    "            comp_item.next_sibling.next_sibling.string] \n",
    "            for comp_item in [div.find_all('b') \n",
    "            for div in self.soup.find_all('div', style=\"margin: 2px;\")][0]\n",
    "        }\n",
    "        \n",
    "        \n",
    "        div = [\n",
    "            item for item in self.soup.find_all('div')\n",
    "        ]\n",
    "        \n",
    "        description = [\n",
    "            description.next_siblings for description in [\n",
    "               item.b.next_sibling for item in div if item.b !=None\n",
    "        ] if description!=None][:3]\n",
    "        \n",
    "        description = [\n",
    "            sub_item.string.strip('\\n').strip('\\t').strip('\\n') \n",
    "            for item in description \n",
    "            for sub_item in item if sub_item.string!=None\n",
    "        ]\n",
    "\n",
    "        comp_data['title']=title; comp_data['description']=description\n",
    "        \n",
    "        titles_c = [\n",
    "           ['missing','missing'] if a_html.a == None else \n",
    "           [a_html.a['href'],a_html.a.get_text()] \n",
    "           for a_html in tables\n",
    "        ]\n",
    "        votes = [\n",
    "            None if table.span ==None else \n",
    "            list(table.span.next_siblings)[-3].get_text() \n",
    "            for table in tables\n",
    "        ]\n",
    "        views_total = [\n",
    "            None if table.span == None else \n",
    "            list(table.span.next_siblings)[1].get_text()\n",
    "            for table in tables\n",
    "        ]\n",
    "        \n",
    "        views = [\n",
    "            None if table.span == None else \n",
    "            list(table.span.next_siblings)[2].get_text()\n",
    "            for table in tables\n",
    "        ]\n",
    "        \n",
    "   \n",
    "        rank = [ None if i.b ==None else \n",
    "           i.b.get_text() for i in self.soup.find_all(\n",
    "            'td', valign=\"top\", width=\"200\")\n",
    "        ]\n",
    "     \n",
    "\n",
    "        self.meta =  { 'titles':titles_c,\n",
    "            'meta_data':meta,'total_means':total_means, \n",
    "            'image_ids':image_id,'url':li, 'competion_details':comp_data,\n",
    "            'rank':rank, 'n_votes':votes,'n_views':views_total,\n",
    "            'views_during_voting':views\n",
    "                     }\n",
    "\n",
    "\n",
    "    def get_data_dict(self):\n",
    "        self.new_id = self.new_challenges\n",
    "        self.all_id = self.all_challenges\n",
    "        self.old_id = np.setdiff1d(self.all_id_challenges, self.new_challenges)\n",
    "        if self.scrape_type==None and 'data_dict' not in vars(self):\n",
    "            with open(self.fid + '.json', 'r') as fid:\n",
    "                self.data_dict = json.load(fid)\n",
    "                self.challs_touched = list(self.data_dict.keys())\n",
    "        self.data_dict['scraped']= { }\n",
    "        rng = np.random.default_rng()\n",
    "        self.new_challenges = rng.permutation(self.new_challenges)\n",
    "        for chall in tqdm(self.new_challenges, position=0, leave=True):\n",
    "            self.chall_id = chall\n",
    "            self.challenge = self.challs_dict[chall]\n",
    "            if chall in self.new_challenges:    \n",
    "                self.data_dict['scraped'][self.chall_id] = self.chall_id in self.new_challenges\n",
    "                self.soup, self.image_pages = self.competition_image_links()\n",
    "                self.text = self.soup.get_text()\n",
    "                if self.soup.string == self.Request_failed or 'Request failed' in self.text:\n",
    "                    self.data_dict[self.chall_id]=self.Request_failed\n",
    "                    self.all_to_dict(success=False)\n",
    "                       \n",
    "                else:\n",
    "                    self.parse_html()\n",
    "                    self.all_to_dict(success=True)\n",
    "                   \n",
    "                test = len(self.new_challenges)\n",
    "                idx_del = np.where(self.new_challenges == self.chall_id)\n",
    "                self.new_challenges = np.delete(self.new_challenges,idx_del)\n",
    "                self.new_challenges = rng.permutation(self.new_challenges)\n",
    "            else:\n",
    "                continue\n",
    "                \n",
    "                    \n",
    "            while len(self.new_challenges)==test:\n",
    "                try:\n",
    "                    with open(self.fid + '.json', 'r') as fid:\n",
    "                        self.data_dict = json.load(fid)\n",
    "                        self.challs_touched = np.array(list(data_dict.keys()))\n",
    "                        self.new_challenges = np.setdiff1d(self.challs_touched, self.all_challenges)\n",
    "\n",
    "                except:\n",
    "                    print('not read')\n",
    "\n",
    "            \n",
    "    \n",
    "    def all_to_dict(self,**kwargs):\n",
    "        if kwargs['success']:\n",
    "            chall_id = self.chall_id\n",
    "            self.data_dict[chall_id]={\n",
    "                self.meta['image_ids'][idx]+'_'+str(idx):\n",
    "                {'urls':{'thumbnail':\n",
    "                         'https:'+ self.meta['url'][idx],\n",
    "                         'full_resolution': 'https:'+\n",
    "                         self.meta['url'][idx].replace('/120/','/1200/')},  \n",
    "                 'meta':{ 'means':self.meta['meta_data'][idx],\n",
    "                          'Overall_Mean':self.meta['total_means'][idx],\n",
    "                          'votes':self.meta['n_votes'][idx],\n",
    "                          'rank':self.meta['rank'][idx],\n",
    "                         'views_all':str(self.meta['n_views'][idx]),\n",
    "                         'view_during_voting':str(self.meta['views_during_voting'][idx])\n",
    "                        },\n",
    "                'image'+self.meta['image_ids'][idx]+'_page':\n",
    "                 self.img_url_base+self.meta['image_ids'][idx].strip('.jpg'),\n",
    "                 'image_title':self.meta['titles'][idx]\n",
    "                } for idx in range(len(self.meta['image_ids']))}\n",
    "            self.data_dict[chall_id]['status'] = {\n",
    "                'scrape_items_match':\n",
    "                len(self.meta['url'])==len(self.meta['n_votes'])==len(self.meta['meta_data']),\n",
    "                'subset_new':chall_id in self.new_id, \n",
    "                'subset_old':chall_id in self.old_id,\n",
    "                'subset_all':chall_id in self.all_id\n",
    "            }\n",
    "            self.data_dict[chall_id]['all_urls'] = {\n",
    "                url.split('=')[-1]:url for url in self.image_pages\n",
    "            }\n",
    "            self.data_dict['scraped'][chall_id] = chall_id in self.new_challenges\n",
    "            self.data_dict[chall_id]['comp_data']=self.meta['competion_details']\n",
    "            self.fid='dp_challenge_dot_com_data_dict'\n",
    "            self.dp_write()\n",
    "        else:\n",
    "            self.data_dict[self.chall_id]={'fail message':self.text}\n",
    "            self.data_dict[self.chall_id]['status'] = {\n",
    "                'subset_new':self.chall_id in self.new_id, \n",
    "                'subset_old':self.chall_id in self.old_id,\n",
    "                'subset_all':self.chall_id in self.all_id\n",
    "            }\n",
    "            self.dp_write()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b219fe92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
